{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ab2326-d77e-417c-a960-793c69e2af82",
   "metadata": {},
   "source": [
    "# üì•üì§Inputs and Outputs (Model I/O)\n",
    "\n",
    "In LangChain, Model I/O (Input/Output) is the layer that defines how you interact with models. It provides standardized abstractions for sending prompts into models and parsing their responses.\n",
    "\n",
    "## üîë Why do we need Model I/O\n",
    "* **Consistency across models**: You can swap between OpenAI, Anthropic, Llama, etc. without changing your business logic, since LangChain standardizes the I/O.\n",
    "* **Structured outputs**: Instead of working with free-form text, you can parse model responses into JSON, Pydantic objects, or enums. This reduces errors and hallucinations.\n",
    "* **Composable pipelines**: Model I/O integrates with other LangChain pieces (retrievers, memory, tools) so you can **chain models and functions easily**.\n",
    "* **Flexibility**:\n",
    "    * Fine control over prompts (system + human messages, variable substitution).\n",
    "    * Choice of parsers (regex, structured, function-calling).\n",
    "    * Input/output transformations before/after the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b301c92d-aef7-46ba-ba2d-3b432d621b97",
   "metadata": {},
   "source": [
    "# üìù Prompt Templates\n",
    "A Prompt template in LangChain is a reusable way to create dynamic prompts for LLMs.\n",
    "Instead of hardcoding the full prompt string, you define a template with placeholders, and later fill those placeholders with variables at runtime.\n",
    "This ensures:\n",
    "\n",
    "* Consistency ‚Äì Reuse the same structure for different inputs.\n",
    "* Flexibility ‚Äì Easily swap variables without rewriting prompts.\n",
    "* Maintainability ‚Äì Keeps your LLM app organized as prompts grow.\n",
    "\n",
    "LangChain Uses two types of prompt templates\n",
    "1. **PromptTemplate** (for completion models)\n",
    "2. **ChatPromptTemplate** (for chat models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1660b1bc-0695-43e9-ac19-d171e5ab8479",
   "metadata": {},
   "source": [
    "## ‚ú®PromptTemplate\n",
    "* **Purpose**: Used for text-based LLMs (completion models).\n",
    "* **Output**: A single string prompt.\n",
    "* **Use case**: When you want to generate a plain text prompt (no roles like ‚Äúsystem‚Äù or ‚Äúuser‚Äù)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01c8ce-55fa-431e-8376-f15a45223d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## google colab\n",
    "# %pip install langchain openai\n",
    "# %pip install langchain langchain-community openai\n",
    "# %pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50f5cf9c-b51b-4e1d-9c1e-328d902c603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9eb6a16-7c73-418c-8cb6-77207b956217",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<Your API KEY>\" # Google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f16a13b3-9f45-463c-9cef-c9773df60c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, api_key=os.getenv(\"OPEN_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88d639d6-c51e-4d9a-8bb5-689eba782cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are three benefits of using LangChain?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Single input variable: \"product\"\n",
    "single_input_template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What are three benefits of using {product}?\"\n",
    ")\n",
    "\n",
    "# template = \"What are three benefits of using {product}?\"\n",
    "# prompt = PromptTemplate.from_template(template)\n",
    "# print(prompt.format(product=\"LangChain\"))\n",
    "\n",
    "# Format with input\n",
    "formatted_prompt = single_input_template.format(product=\"LangChain\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1a5b3cd-a0fc-4447-8ecc-27654677444e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response: \n",
      "\n",
      "1. Increased Efficiency: LangChain utilizes blockchain technology to automate and streamline language translation processes, resulting in increased efficiency and reduced turnaround times. This can save businesses time and resources, allowing them to focus on other important tasks.\n",
      "\n",
      "2. Enhanced Security: With its decentralized and immutable nature, LangChain offers enhanced security for language translation services. This means that translations are less susceptible to tampering or manipulation, ensuring the accuracy and integrity of the translated content.\n",
      "\n",
      "3. Cost Savings: By eliminating intermediaries and automating processes, LangChain can significantly reduce the costs associated with language translation services. This makes it a more affordable option for businesses and individuals looking to translate content into multiple languages.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(formatted_prompt)\n",
    "print(\"LLM response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9aee05-abc1-404b-af60-f3f1fb56b1f8",
   "metadata": {},
   "source": [
    "## üí¨ChatPromptTemplate\n",
    "* **Purpose**: Used for chat-based LLMs (like OpenAI‚Äôs GPT-4, GPT-4o, GPT-3.5).\n",
    "* **Output**: A list of message objects (with roles: system, human, ai, etc.).\n",
    "* **Use case**: When you need structured multi-role prompts (system instructions + user queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56099d17-7e85-4eab-a272-0acbcff33e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document as a string\n",
    "document_text = \"\"\"\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "It provides abstractions for prompts, chains, agents, memory, and integrates with many external tools.\n",
    "\"\"\"\n",
    "\n",
    "# Define the chat prompt template with system + human messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use the provided document to answer questions.\"),\n",
    "    (\"human\", \"Here is the document:\\n{document}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Format the prompt with input variables\n",
    "messages = chat_prompt.format_messages(\n",
    "    document=document_text,\n",
    "    question=\"What does LangChain provide?\"\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=os.getenv(\"OPEN_API_KEY\"))\n",
    "\n",
    "# Get the response\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07692f6a-09af-4d94-a332-6cd383b7b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc07fe6-7e32-4ef7-a156-561f03db7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document string\n",
    "document_text = \"\"\"\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "It provides abstractions for prompts, chains, agents, memory, and integrates with many external tools.\n",
    "\"\"\"\n",
    "\n",
    "# Create system and human message prompt templates\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Use the provided document to answer questions.\"\n",
    ")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"Here is the document:\\n{document}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# Build the chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Format with inputs\n",
    "messages = chat_prompt.format_messages(\n",
    "    document=document_text,\n",
    "    question=\"What does LangChain provide?\"\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0,  api_key=os.getenv(\"OPEN_API_KEY\"))\n",
    "\n",
    "# Invoke the model\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bcd791-de57-41a7-ab4c-30e69821fc1a",
   "metadata": {},
   "source": [
    "# Few Shots Prompt-Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf3a7cf-de6a-48c4-b460-67de731c492a",
   "metadata": {},
   "source": [
    "In LangChain, a few-shot template is a way to guide a language model by giving it examples of how it should respond before asking the real question. The idea comes from few-shot learning, where instead of training the model with lots of data, you show it just a handful of demonstrations right inside the prompt.\n",
    "\n",
    "LangChain makes this easier with **FewShotPromptTemplate**. Instead of manually writing out all examples in one long string, you define a template for the examples and then let LangChain format them dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "301409a5-d0bd-45ad-90e4-24077536f63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    FewShotPromptTemplate, \n",
    "    FewShotChatMessagePromptTemplate, \n",
    "    PromptTemplate, \n",
    "    ChatPromptTemplate, \n",
    "    HumanMessagePromptTemplate, \n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31653847-9597-458e-89f4-eaf36e21d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=os.getenv(\"OPEN_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8510b8ff-b28e-4fa7-b163-06b12a6f9bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I loved the storyline and the acting was brilliant!\n",
      "Sentiment: Positive\n",
      "\n",
      "\n",
      "Review: The movie was too long and really boring.\n",
      "Sentiment: Negative\n",
      "\n",
      "\n",
      "Review: Amazing visuals and great soundtrack.\n",
      "Sentiment: Positive\n",
      "\n",
      "\n",
      "Review: The plot was confusing and poorly executed.\n",
      "Sentiment:\n",
      "=== Final Prompt Sent to LLM ===\n",
      "Review: I loved the storyline and the acting was brilliant!\n",
      "Sentiment: Positive\n",
      "\n",
      "\n",
      "Review: The movie was too long and really boring.\n",
      "Sentiment: Negative\n",
      "\n",
      "\n",
      "Review: Amazing visuals and great soundtrack.\n",
      "Sentiment: Positive\n",
      "\n",
      "\n",
      "Review: The plot was confusing and poorly executed.\n",
      "Sentiment:\n",
      "\n",
      "=== Model Output ===\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# Template for each example\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"review\", \"sentiment\"],\n",
    "    template=\"Review: {review}\\nSentiment: {sentiment}\\n\"\n",
    ")\n",
    "\n",
    "# Few-shot examples\n",
    "examples = [\n",
    "    {\"review\": \"I loved the storyline and the acting was brilliant!\", \"sentiment\": \"Positive\"},\n",
    "    {\"review\": \"The movie was too long and really boring.\", \"sentiment\": \"Negative\"},\n",
    "    {\"review\": \"Amazing visuals and great soundtrack.\", \"sentiment\": \"Positive\"},\n",
    "]\n",
    "\n",
    "# FewShotPromptTemplate with suffix for new input\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Review: {input}\\nSentiment:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# Format with a new review\n",
    "print(few_shot_prompt.format(input=\"The plot was confusing and poorly executed.\"))\n",
    "# 4. Instantiate an LLM\n",
    "\n",
    "\n",
    "# 5. Format the prompt with a new review\n",
    "formatted_prompt = few_shot_prompt.format(input=\"The plot was confusing and poorly executed.\")\n",
    "print(\"=== Final Prompt Sent to LLM ===\")\n",
    "print(formatted_prompt)\n",
    "\n",
    "# 6. Call the LLM\n",
    "response = llm.invoke(formatted_prompt)\n",
    "print(\"\\n=== Model Output ===\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd4fcce9-2e4a-4e83-bc84-d3dd092a7958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Prompt Messages ===\n",
      "SYSTEM: You are a helpful assistant that classifies movie reviews as Positive or Negative.\n",
      "HUMAN: Review: I loved the storyline and the acting was brilliant!\n",
      "AI: Positive\n",
      "HUMAN: Review: The movie was too long and really boring.\n",
      "AI: Negative\n",
      "HUMAN: Review: Amazing visuals and great soundtrack.\n",
      "AI: Positive\n",
      "HUMAN: Review: The plot was confusing and poorly executed.\n",
      "\n",
      "=== Model Output ===\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "# 1) Build a chat prompt for each example (human ‚Üí ai)\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Review: {review}\"),\n",
    "    (\"ai\", \"{sentiment}\"),\n",
    "])\n",
    "\n",
    "# 2) Few-shot examples\n",
    "chat_examples = [\n",
    "    {\"review\": \"I loved the storyline and the acting was brilliant!\", \"sentiment\": \"Positive\"},\n",
    "    {\"review\": \"The movie was too long and really boring.\", \"sentiment\": \"Negative\"},\n",
    "    {\"review\": \"Amazing visuals and great soundtrack.\", \"sentiment\": \"Positive\"},\n",
    "]\n",
    "\n",
    "# 3) Few-shot block (no input_variables here)\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=chat_examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "# 4) Full chat prompt (system + few-shot + new human input)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that classifies movie reviews as Positive or Negative.\"),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"Review: {input}\"),\n",
    "])\n",
    "\n",
    "# 6) Format and invoke\n",
    "messages = chat_prompt.format_messages(input=\"The plot was confusing and poorly executed.\")\n",
    "print(\"=== Final Prompt Messages ===\")\n",
    "for m in messages:\n",
    "    print(m.type.upper() + \":\", m.content)\n",
    "\n",
    "resp = llm.invoke(messages)\n",
    "print(\"\\n=== Model Output ===\")\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e040b1-99ce-49cf-875b-bf14569c4a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
