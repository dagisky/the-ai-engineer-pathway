{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04705c1a-9c03-436e-9dd0-3c80d1764925",
   "metadata": {},
   "source": [
    "# Cache\n",
    "LangChain cache is a feature that helps speed up responses and reduce costs by storing the outputs of LLM calls so you don’t need to recompute them every time.\n",
    "\n",
    "#### Why use cache?\n",
    "* Avoid repeated API calls for the same input.\n",
    "* Save tokens (and cost).\n",
    "* Improve performance (faster responses).\n",
    "#### How it works\n",
    "* When you pass a prompt to the LLM, LangChain first checks the cache.\n",
    "* If the input has been seen before, it immediately returns the stored response.\n",
    "* If not, it calls the LLM, stores the result, and then returns it.\n",
    "\n",
    "#### Supported Backends\n",
    "\n",
    "* In-memory (fast, resets when process ends).\n",
    "* SQLite (persistent, lightweight).\n",
    "* Redis (distributed, scalable).\n",
    "\n",
    "Other custom caches can be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fbcb57-1950-4cc7-9f9a-42abd5803e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.cache import InMemoryCache\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9250c26-e5a0-4e96-8cb7-9be8cd9fa75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain.llm_cache = InMemoryCache()\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caeac658-84f5-4322-bae9-7c894c65037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call: \n",
      "\n",
      "Why did the programming language go on a diet?\n",
      "\n",
      "Because it wanted to be a lean, mean LangChain machine!\n",
      "Second call (cached): \n",
      "\n",
      "Why did the programming language go on a diet?\n",
      "\n",
      "Because it wanted to be a lean, mean LangChain machine!\n"
     ]
    }
   ],
   "source": [
    "# Enable in-memory caching\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# Initialize an LLM (replace with your API key if needed)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, api_key=os.getenv(\"OPEN_API_KEY\"))\n",
    "\n",
    "# First call → will query the API\n",
    "response1 = llm.invoke(\"Tell me a joke about LangChain\")\n",
    "print(\"First call:\", response1)\n",
    "\n",
    "# Second call with the same prompt → served from cache\n",
    "response2 = llm.invoke(\"Tell me a joke about LangChain\")\n",
    "print(\"Second call (cached):\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f9c9a-09a1-4735-a087-5d03b3630eef",
   "metadata": {},
   "source": [
    "### PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e180665-09e5-41af-b1e3-020c09696125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call: \n",
      "\n",
      "Why did the programming language go on a diet?\n",
      "\n",
      "Because it wanted to be a lean, mean LangChain machine!\n",
      "Second call (cached): \n",
      "\n",
      "Why did the programming language go on a diet?\n",
      "\n",
      "Because it wanted to be a lean, mean LangChain machine!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# Enable in-memory cache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# Initialize LLM\n",
    "\n",
    "# Define a simple prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    ")\n",
    "\n",
    "# Format the prompt\n",
    "formatted_prompt = prompt.format(topic=\"LangChain\")\n",
    "\n",
    "# First call → API hit\n",
    "response1 = llm.invoke(formatted_prompt)\n",
    "print(\"First call:\", response1)\n",
    "\n",
    "# Second call with the same prompt → cache hit\n",
    "response2 = llm.invoke(formatted_prompt)\n",
    "print(\"Second call (cached):\", response2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bb4c65-2205-4d9a-89d3-4d14b1a6a713",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3b1a7eb-56c2-4589-ba9e-cc12a2f39d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call: \n",
      "\n",
      "System: Why did the blockchain developer switch to LangChain? Because he wanted to make his code more secure and unbreakable!\n",
      "Second call (cached): \n",
      "\n",
      "System: Why did the blockchain developer switch to LangChain? Because he wanted to make his code more secure and unbreakable!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "# Enable in-memory cache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# Define chat-style prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
    "    (\"human\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "# Format with input\n",
    "formatted_chat_prompt = chat_prompt.format_messages(topic=\"LangChain\")\n",
    "\n",
    "# First call → API hit\n",
    "response1 = llm.invoke(formatted_chat_prompt)\n",
    "print(\"First call:\", response1)\n",
    "\n",
    "# Second call with the same prompt → cache hit\n",
    "response2 = llm.invoke(formatted_chat_prompt)\n",
    "print(\"Second call (cached):\", response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a92f81-6e10-4359-a30a-005c29c3a646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
