{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04705c1a-9c03-436e-9dd0-3c80d1764925",
   "metadata": {},
   "source": [
    "## Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7fbcb57-1950-4cc7-9f9a-42abd5803e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.cache import InMemoryCache\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9250c26-e5a0-4e96-8cb7-9be8cd9fa75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.llm_cache = InMemoryCache()\n",
    "load_dotenv()\n",
    "open_api_key = os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caeac658-84f5-4322-bae9-7c894c65037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call: \n",
      "\n",
      "Why did the programming language go on a diet?\n",
      "\n",
      "Because it wanted to be a lean, mean LangChain machine!\n",
      "Second call (cached): \n",
      "\n",
      "Why did the programming language go on a diet?\n",
      "\n",
      "Because it wanted to be a lean, mean LangChain machine!\n"
     ]
    }
   ],
   "source": [
    "# Enable in-memory caching\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# Initialize an LLM (replace with your API key if needed)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo-instruct\", temperature=0, api_key=open_api_key)\n",
    "\n",
    "# First call → will query the API\n",
    "response1 = llm.invoke(\"Tell me a joke about LangChain\")\n",
    "print(\"First call:\", response1)\n",
    "\n",
    "# Second call with the same prompt → served from cache\n",
    "response2 = llm.invoke(\"Tell me a joke about LangChain\")\n",
    "print(\"Second call (cached):\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad0bf0-77da-48d1-b625-ab0746c5cd47",
   "metadata": {},
   "source": [
    "## Prompttemplate\n",
    "* Purpose: Used for text-based LLMs (completion models).\n",
    "* Output: A single string prompt.\n",
    "* Use case: When you want to generate a plain text prompt (no roles like “system” or “user”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "648b224d-ed83-4e1c-82d4-bd85a09053cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are three benefits of using LangChain?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Single input variable: \"product\"\n",
    "single_input_template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What are three benefits of using {product}?\"\n",
    ")\n",
    "\n",
    "# Format with input\n",
    "formatted_prompt = single_input_template.format(product=\"LangChain\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bec3e5-44cc-426b-a4dc-b93ffab3cad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a positive review about LangChain.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Multiple input variables: \"adjective\" and \"product\"\n",
    "multi_input_template = PromptTemplate(\n",
    "    input_variables=[\"adjective\", \"product\"],\n",
    "    template=\"Write a {adjective} review about {product}.\"\n",
    ")\n",
    "\n",
    "# Format with inputs\n",
    "formatted_prompt = multi_input_template.format(adjective=\"positive\", product=\"LangChain\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c2953-d90e-4459-8991-454760835fa7",
   "metadata": {},
   "source": [
    "## ChatPromptTemplate\n",
    "* Purpose: Used for chat-based LLMs (like OpenAI’s GPT-4, GPT-4o, GPT-3.5).\n",
    "* Output: A list of message objects (with roles: system, human, ai, etc.).\n",
    "* Use case: When you need structured multi-role prompts (system instructions + user queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2c0cae8-c102-4e0e-ad45-d7f2d5cebc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc6df6e4-f744-4aa4-891f-7abd27a6d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "open_api_key = os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2382af3-34a3-4466-9beb-08d0b225c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain provides abstractions for prompts, chains, agents, memory, and integrates with many external tools.\n"
     ]
    }
   ],
   "source": [
    "# Document as a string\n",
    "document_text = \"\"\"\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "It provides abstractions for prompts, chains, agents, memory, and integrates with many external tools.\n",
    "\"\"\"\n",
    "\n",
    "# Define the chat prompt template with system + human messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use the provided document to answer questions.\"),\n",
    "    (\"human\", \"Here is the document:\\n{document}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "# Format the prompt with input variables\n",
    "messages = chat_prompt.format_messages(\n",
    "    document=document_text,\n",
    "    question=\"What does LangChain provide?\"\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=open_api_key)\n",
    "\n",
    "# Get the response\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f6eeebe-adb3-4383-930c-ce3408dd5ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb266d87-9e17-4f3f-9749-54179678b6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain provides abstractions for prompts, chains, agents, memory, and integrates with many external tools.\n"
     ]
    }
   ],
   "source": [
    "# Document string\n",
    "document_text = \"\"\"\n",
    "LangChain is a framework for developing applications powered by language models.\n",
    "It provides abstractions for prompts, chains, agents, memory, and integrates with many external tools.\n",
    "\"\"\"\n",
    "\n",
    "# Create system and human message prompt templates\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Use the provided document to answer questions.\"\n",
    ")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"Here is the document:\\n{document}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "\n",
    "# Build the chat prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
    "\n",
    "# Format with inputs\n",
    "messages = chat_prompt.format_messages(\n",
    "    document=document_text,\n",
    "    question=\"What does LangChain provide?\"\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0,  api_key=open_api_key)\n",
    "\n",
    "# Invoke the model\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c403af-a26a-4a75-b0f7-2cd1a551bda2",
   "metadata": {},
   "source": [
    "## Few Shots Prompt-Template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8229b173-6ae4-42e4-8df8-cc50d8432967",
   "metadata": {},
   "source": [
    "### The Rule Detective: A Few-Shot Learning Example with LangChain\n",
    "The \"Rule Detective\" game is a perfect, simplified metaphor for few-shot learning. The \"Rule Master\" acts as the data provider, giving a few examples, and the \"Rule Detective\" acts as the language model, inferring the underlying pattern.\n",
    "\n",
    "Here's how to structure this game's logic using LangChain's prompt templates.\n",
    "\n",
    "1. The System Prompt: The \"Game Rules\"\n",
    "The SystemMessagePromptTemplate establishes the persona and core instructions for the \"model.\" In our game, this sets the stage for the AI's role and the objective.\n",
    "\n",
    "Prompt Template Logic: The system message tells the AI its identity and the rules it must follow to be a successful \"Rule Detective.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4163a73-3491-4a8c-9373-fae42c1a589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb02cddb-ec7f-4d3e-9ae0-e37ecd6c822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_rule_master_game():\n",
    "    \"\"\"\n",
    "    This game simulates the \"Rule Master\" for a few-shot learning demonstration.\n",
    "    The user acts as the \"Rule Detective\" to guess a hidden rule based on examples.\n",
    "    \"\"\"\n",
    "    # 1. Define the possible rules and their examples\n",
    "    rules_data = [\n",
    "        {\n",
    "            \"rule\": \"All items are round.\",\n",
    "            \"examples\": [\"ball\", \"orange\", \"moon\"],\n",
    "            \"non_examples\": [\"book\", \"chair\", \"pencil\"]\n",
    "        },\n",
    "        {\n",
    "            \"rule\": \"All items can be found in a kitchen.\",\n",
    "            \"examples\": [\"spoon\", \"bowl\", \"refrigerator\"],\n",
    "            \"non_examples\": [\"car\", \"tree\", \"shoe\"]\n",
    "        },\n",
    "        {\n",
    "            \"rule\": \"All items are made of metal.\",\n",
    "            \"examples\": [\"coin\", \"key\", \"wrench\"],\n",
    "            \"non_examples\": [\"grape\", \"plastic bottle\", \"cloud\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # 2. Randomly select a secret rule for the game\n",
    "    secret_rule_data = random.choice(rules_data)\n",
    "    secret_rule = secret_rule_data[\"rule\"]\n",
    "    few_shot_examples = secret_rule_data[\"examples\"]\n",
    "    \n",
    "    print(\"Welcome to The Rule Master Game!\")\n",
    "    print(\"I have a secret rule in mind. Your job is to figure it out.\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 3. Present the System Prompt (The Game Rules)\n",
    "    system_message = SystemMessagePromptTemplate.from_template(\n",
    "        \"You are an expert logician and a brilliant detective. Your task is to identify a secret rule that I have in my mind. I will give you a few examples of items that follow this rule, and then I will ask you about other items. Your final goal is to guess the rule based on the examples and our conversation.\"\n",
    "    )\n",
    "    print(f\"**System Prompt:**\\n{system_message.prompt.template}\\n\")\n",
    "\n",
    "    # 4. Present the Few-Shot Examples (The Training Data)\n",
    "    print(\"**Few-Shot Examples:**\")\n",
    "    for item in few_shot_examples:\n",
    "        human_message = HumanMessagePromptTemplate.from_template(f\"A **{item}** follows the rule.\")\n",
    "        print(f\" - {human_message.prompt.template}\")\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "    # 5. The Conversation Loop (Inference)\n",
    "    print(\"Let the guessing begin!\")\n",
    "    print(\"Type an item to ask if it follows the rule. Type 'guess' to make your final guess.\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nYour item or action: \").strip().lower()\n",
    "\n",
    "        if user_input == 'guess':\n",
    "            final_guess = input(\"What is your final guess for the rule? \")\n",
    "            if final_guess.strip().lower() == secret_rule.strip().lower():\n",
    "                print(\"\\nCongratulations! You are a master detective! You guessed the rule correctly.\")\n",
    "            else:\n",
    "                print(\"\\nThat's a good guess, but it's not the secret rule. The rule was:\")\n",
    "                print(f\"**{secret_rule}**\")\n",
    "            break\n",
    "\n",
    "        # Check if the user's item follows the rule\n",
    "        if user_input in secret_rule_data[\"examples\"] or user_input in secret_rule_data[\"non_examples\"]:\n",
    "            if user_input in secret_rule_data[\"examples\"]:\n",
    "                print(f\"My answer is: Yes, a **{user_input}** follows the rule.\")\n",
    "            else:\n",
    "                print(f\"My answer is: No, a **{user_input}** does not follow the rule.\")\n",
    "        else:\n",
    "            print(\"That item is not in my knowledge base. Please try another one.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c62bc81b-6510-498d-a9f6-a388d82491b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to The Rule Master Game!\n",
      "I have a secret rule in mind. Your job is to figure it out.\n",
      "----------------------------------------\n",
      "**System Prompt:**\n",
      "You are an expert logician and a brilliant detective. Your task is to identify a secret rule that I have in my mind. I will give you a few examples of items that follow this rule, and then I will ask you about other items. Your final goal is to guess the rule based on the examples and our conversation.\n",
      "\n",
      "**Few-Shot Examples:**\n",
      " - A **ball** follows the rule.\n",
      " - A **orange** follows the rule.\n",
      " - A **moon** follows the rule.\n",
      "\n",
      "----------------------------------------\n",
      "Let the guessing begin!\n",
      "Type an item to ask if it follows the rule. Type 'guess' to make your final guess.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Your item or action:  ball, orange, moon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That item is not in my knowledge base. Please try another one.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplay_rule_master_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mplay_rule_master_game\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mType an item to ask if it follows the rule. Type \u001b[39m\u001b[33m'\u001b[39m\u001b[33mguess\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to make your final guess.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     user_input = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mYour item or action: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip().lower()\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m user_input == \u001b[33m'\u001b[39m\u001b[33mguess\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     55\u001b[39m         final_guess = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWhat is your final guess for the rule? \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\STUDY\\the-ai-engineer-pathway\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1275\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1273\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\STUDY\\the-ai-engineer-pathway\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py:1320\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1319\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "play_rule_master_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e180665-09e5-41af-b1e3-020c09696125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
