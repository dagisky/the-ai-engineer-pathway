{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5c6e44-ab95-44f0-95c7-fc83712290a9",
   "metadata": {},
   "source": [
    "# Chains\n",
    "#### What is a Chain?\n",
    "\n",
    "A chain in LangChain is a pipeline of components where the output of one step automatically becomes the input of the next. Instead of manually connecting functions together, LangChain standardizes the way prompts, models, parsers, and retrievers interact. \n",
    "\n",
    "#### Chains vs Agents\n",
    "\n",
    "Chains and agents both help coordinate multiple LLM calls, but they differ in behavior. \n",
    "* A chain follows a fixed path that you define at design time. \n",
    "* An agent is dynamic: it decides at runtime which tool or path to take, depending on the input.\n",
    "\n",
    "Chains are simpler, more deterministic, while agents provide flexibility at the cost of predictability\n",
    "\n",
    "#### Why Chains Matter\n",
    "\n",
    "* Reusability,\n",
    "* Maintainability\n",
    "* Possible to add retries, caching, fallbacks or tracing with out changing the main workflow logic\n",
    "* Enforce consistency\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0e72c-7019-43e6-94d0-78315b310ee2",
   "metadata": {},
   "source": [
    "## Langchain Expression Language\n",
    "**The LangChain Expression Language (LCEL)** is a declarative way to compose components. It was developed to address limitations of the old Chain class system. LCEL provides several advantages:\n",
    "* **Streaming Support**: LCEL makes it easier to stream outputs from each step of your sequence, even when you are chaining calls. The older chains required using callbacks for streaming.\n",
    "* **Parallelism**: You can easily run multiple parts of a chain in parallel with LCEL, which can improve the performance of your application.\n",
    "* **Debugging and Visibility**: LCEL provides better  visibility into the structure of your application, making it easier to debug and inspect the intermediate steps of a chain.\n",
    "* **Standard Interface**: LCEL introduced the Runnable protocol, a standard interface that all components adhere to. SimpleSequentialChain was an early implementation of this concep\n",
    "\n",
    "LCEL is the modern standard for building chains. Its key idea is `composability`: every component in LangChain implements a `Runnable` interface, and you can combine them using the pipe operator `(|)`. This makes chain definitions short and expressive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b98298-c8f8-4ad7-ae48-de3316e034d1",
   "metadata": {},
   "source": [
    "### RunnableSequence (Sequential Chains)\n",
    "A `RunnableSequence` is the simplest and most common type of chain in LangChain. It defines a step-by-step pipeline where the output of one component becomes the input of the next. Think of it as a conveyor belt: input enters, gets transformed by each stage in order, and finally produces the output.\n",
    "\n",
    "In LCEL, the most natural way to build a sequential chain is with the `|` operator, which implicitly creates a RunnableSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997f1b63-aed6-4d54-b0a8-3c925cdd1c25",
   "metadata": {},
   "source": [
    "#### Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed671b2e-b4b3-4a73-8baf-83b6077b0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
    "from langchain_core.output_parsers import PydanticOutputParser, JsonOutputParser\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02b2af75-698e-40e9-94ea-8d13f2d2020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7, api_key=os.getenv(\"OPEN_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1aacbfc0-2716-4d0d-8142-6abca34f1a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't LangChain models ever get lost?\n",
      "\n",
      "Because they always follow the \"chain of command\"!\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a friendly assistant.\"),\n",
    "    (\"human\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "# chain = RunnableSequence([prompt, llm, parser])\n",
    "\n",
    "result = chain.invoke({\"topic\": \"LangChain\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20321efc-30d6-4491-a995-8bc4e7aab8a8",
   "metadata": {},
   "source": [
    "#### Using JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd0a7a0e-c2a6-4e01-8825-0ac73d587830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': 'Why did the LangChain developer bring a ladder to the library? Because they heard the books were stacked with high-level abstractions!'}\n"
     ]
    }
   ],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline of the joke\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a comedian AI. Output only valid JSON.\"),\n",
    "    (\"human\", \"Write a joke about {topic}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "print(chain.invoke({\"topic\": \"LangChains\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc08fec8-ae87-40cc-84dd-c2f3fb9ac48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'setup': 'Why did the LangChain break up with its library?', 'punchline': \"It couldn't handle the lack of support!\"}\n"
     ]
    }
   ],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline of the joke\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a comedian AI. Output only valid JSON.\"),\n",
    "    (\"human\", \"Write a joke about {topic}\\n\"\n",
    "    \"{format_instructions}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "print(chain.invoke({\"topic\": \"LangChains\", \"format_instructions\": format_instructions}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f89c1b-e618-465c-8db8-99b1842fee99",
   "metadata": {},
   "source": [
    "#### Expanding with More Steps\n",
    "\n",
    "Since steps are modular, you can insert transformations before or after the LLM. For example, preprocessing input to uppercase before sending to the model, or postprocessing to count tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53c02ae2-e69b-4552-b395-d2fa1a0bfd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup='Why did the LangChain break up with its AI partner?' punchline=\"It couldn't handle the lack of context in their relationship!\"\n",
      "setup=\"Why don't LangChain developers play hide and seek? ðŸš€\" punchline=\"Why don't LangChain developers play hide and seek? ðŸš€ \"\n"
     ]
    }
   ],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline of the joke\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a comedian AI. Output only valid JSON.\"),\n",
    "    (\"human\", \"Write a joke about {topic}\\n\"\n",
    "    \"{format_instructions}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "print(chain.invoke({\"topic\": \"LangChains\", \"format_instructions\": format_instructions}))\n",
    "\n",
    "def to_upper(inputs: dict) -> dict:\n",
    "    inputs[\"topic\"].upper()\n",
    "    return inputs\n",
    "\n",
    "# uppercase = RunnableLambda(lambda x: {**x, \"topic\": x[\"topic\"].upper()})\n",
    "\n",
    "def add_exclamation(output: Joke) -> str:\n",
    "    output.setup = output.setup + \" ðŸš€\"\n",
    "    output.punchline = output.setup + \" \"\n",
    "    return output\n",
    "\n",
    "chain = RunnableSequence(to_upper, prompt, llm, parser, add_exclamation)\n",
    "\n",
    "print(chain.invoke({\"topic\": \"LangChain\", \"format_instructions\": format_instructions}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474171a6-95c6-4983-b2d1-538688809293",
   "metadata": {},
   "source": [
    "### RunnableMap\n",
    "A `RunnableMap` is a chain that executes multiple branches in parallel. Instead of sending your input through one fixed pipeline, you can fork the input into several independent sub-chains at the same time. Each sub-chain processes the same input (or a portion of it), and the results are collected into a dictionary keyed by branch names.\n",
    "\n",
    "This pattern is useful when you want to extract multiple types of information from the same input in a single cal\n",
    "##### Example\n",
    "Generating a summary, extracting keywords, and classifying sentiment all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f92537ba-c4ad-4e5e-92f2-25b99e4ecc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0bca1aef-7c2e-4616-8638-9030a5777958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'LangChain is a framework designed to assist developers in creating applications using large language models by offering composable abstractions.', 'keywords': '1. LangChain\\n2. Framework\\n3. Developers\\n4. Applications\\n5. Language Models'}\n"
     ]
    }
   ],
   "source": [
    "# Branch 1: Summarization\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a summarizer.\"),\n",
    "    (\"human\", \"Summarize the following text:\\n\\n{text}\")\n",
    "])\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Branch 2: Keyword extraction\n",
    "keyword_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a keyword extractor.\"),\n",
    "    (\"human\", \"Extract 5 keywords from the following text:\\n\\n{text}\")\n",
    "])\n",
    "keyword_chain = keyword_prompt | llm | StrOutputParser()\n",
    "\n",
    "# RunnableMap to run both in parallel\n",
    "map_chain = RunnableMap({\n",
    "    \"summary\": summary_chain,\n",
    "    \"keywords\": keyword_chain\n",
    "})\n",
    "\n",
    "doc = \"LangChain is a framework that helps developers build applications with large language models by providing composable abstractions.\"\n",
    "result = map_chain.invoke({\"text\": doc})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b70000-ba58-416f-9207-c5ca2a8a0ebd",
   "metadata": {},
   "source": [
    "#### Note \n",
    "Branches donâ€™t need to return the same type. One branch can return a string, another a list, another a JSON object. The results will still be collected into a dictionary.\n",
    "\n",
    "**For example**: you could have:\n",
    "\n",
    "* summary â†’ plain string\n",
    "* keywords â†’ list of strings\n",
    "* classification â†’ structured JSON (via Pydantic)\n",
    "\n",
    "This flexibility makes RunnableMap powerful for building pipelines that produce rich, multi-field outputs in a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ffe6d81-aa06-4197-ad26-764a608a6c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'LangChain is a framework designed to assist developers in creating applications using large language models by offering modular and composable abstractions.', 'keywords': '1. LangChain  \\n2. framework  \\n3. developers  \\n4. applications  \\n5. large language models', 'original_word_count': 17, 'summary_word_count': 21}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "# Branch 1: Summarization\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a summarizer.\"),\n",
    "    (\"human\", \"Summarize the following text:\\n\\n{text}\")\n",
    "])\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Branch 2: Keyword extraction\n",
    "keyword_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a keyword extractor.\"),\n",
    "    (\"human\", \"Extract 5 keywords from the following text:\\n\\n{text}\")\n",
    "])\n",
    "keyword_chain = keyword_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Helper: count words in a string\n",
    "def word_count_text(inputs: dict) -> int:\n",
    "    return len(inputs[\"text\"].split())\n",
    "\n",
    "def word_count_summary(inputs: dict) -> int:\n",
    "    return len(inputs[\"summary\"].split())\n",
    "\n",
    "# RunnableMap to run all in parallel\n",
    "map_chain = RunnableMap({\n",
    "    \"summary\": summary_chain,\n",
    "    \"keywords\": keyword_chain,\n",
    "    \"original_word_count\": word_count_text,\n",
    "    # NOTE: this depends on summary, so weâ€™ll wrap with a lambda inside another map\n",
    "})\n",
    "\n",
    "# First, run summary + keywords + original_word_count\n",
    "partial_result = map_chain.invoke({\"text\": doc})\n",
    "\n",
    "# Then compute word_count of the summary\n",
    "partial_result[\"summary_word_count\"] = len(partial_result[\"summary\"].split())\n",
    "\n",
    "print(partial_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5db0ef-8332-4cea-aff9-100e2e457c6b",
   "metadata": {},
   "source": [
    "### Hierarchical composition\n",
    "This is the idea that you can nest maps and sequences inside each other to build multi-level pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9fb7f992-bd90-4f02-9e29-29afe07bd0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'LangChain is a framework designed to assist developers in creating applications using large language models by offering composable abstractions.', 'keywords': '1. LangChain\\n2. framework\\n3. developers\\n4. applications\\n5. language models', 'original_word_count': 17, 'summary_word_count': 19}\n"
     ]
    }
   ],
   "source": [
    "# --- Branch 1: summary ---\n",
    "summary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a summarizer.\"),\n",
    "    (\"human\", \"Summarize the following text:\\n\\n{text}\")\n",
    "])\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --- Branch 2: keywords ---\n",
    "keyword_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a keyword extractor.\"),\n",
    "    (\"human\", \"Extract 5 keywords from the following text:\\n\\n{text}\")\n",
    "])\n",
    "keyword_chain = keyword_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --- Parallel fan-out (both branches see the same input dict) ---\n",
    "map_chain = RunnableMap({\n",
    "    \"summary\": summary_chain,\n",
    "    \"keywords\": keyword_chain,\n",
    "    \"original_word_count\": lambda inputs: len(inputs[\"text\"].split()),\n",
    "})\n",
    "\n",
    "# --- Post-process step that depends on the summary produced by map_chain ---\n",
    "def add_summary_wc(outputs: dict) -> dict:\n",
    "    # outputs looks like: {\"summary\": \"...\", \"keywords\": \"...\", \"original_word_count\": int}\n",
    "    return {\n",
    "        **outputs,\n",
    "        \"summary_word_count\": len(outputs[\"summary\"].split())\n",
    "    }\n",
    "\n",
    "# Option A: Explicit RunnableSequence\n",
    "seq = RunnableSequence(map_chain, add_summary_wc)\n",
    "\n",
    "# Option B: LCEL pipe shorthand (equivalent)\n",
    "# seq = map_chain | add_summary_wc\n",
    "\n",
    "doc = \"LangChain is a framework that helps developers build applications with large language models by providing composable abstractions.\"\n",
    "result = seq.invoke({\"text\": doc})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9b8011-7a4c-42ef-aaca-6b5b0dd57797",
   "metadata": {},
   "source": [
    "# Depricated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dfec63-7d06-430e-bf68-26590a80f98b",
   "metadata": {},
   "source": [
    "## LLMChain (depricated)\n",
    "* LLMChain is a foundational building block or component within the LangChain framework that combines a Large Language Model (LLM) with a prompt template.\n",
    "  * **Format user input**: The prompt template structures the input for the LLM.\n",
    "  * **Call the LLM**: The formatted prompt is sent to the LLM for processing.\n",
    "  * **Parse the output**: The raw response from the LLM is transformed into a more usable format by the output parser.\n",
    "* Deprecated since version 0.1.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2956b0b5-d21c-43ed-867c-ee78cb937440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import json\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "999a248f-0b9d-4329-a7ea-0df36e53aaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain output type: <class '__main__.MovieSuggestion'>\n",
      "title='Mad Max: Fury Road' genre='Action' reason='This movie is a high-octane action adventure set in a post-apocalyptic world, filled with thrilling chase sequences and explosive battles, making it perfect for an adventurous mood.'\n",
      "\n",
      "Title: Mad Max: Fury Road\n",
      "Genre: Action\n",
      "Reason: This movie is a high-octane action adventure set in a post-apocalyptic world, filled with thrilling chase sequences and explosive battles, making it perfect for an adventurous mood.\n"
     ]
    }
   ],
   "source": [
    "# Pydantic model for structured output\n",
    "class MovieSuggestion(BaseModel):\n",
    "    title: str = Field(description=\"The title of the movie.\")\n",
    "    genre: str = Field(description=\"The genre of the movie.\")\n",
    "    reason: str = Field(description=\"A brief explanation of why the movie was suggested.\")\n",
    "\n",
    "# Set up the parser\n",
    "parser = PydanticOutputParser(pydantic_object=MovieSuggestion)\n",
    "\n",
    "# Define the message templates\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful AI assistant that recommends a movie based on the user's mood. \"\n",
    "    \"Always provide a single recommendation.\"\n",
    ")\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(\n",
    "    \"I'm in the mood for something {mood}. I want my recommendation to be a {genre}. \"\n",
    "    \"\\n\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "# Combine the message templates into a ChatPromptTemplate\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    system_message_template,\n",
    "    human_message_template\n",
    "])\n",
    "\n",
    "# Create the LLMChain with the ChatPromptTemplate\n",
    "# The output parser is passed to the chain constructor.\n",
    "# This approach is less elegant than LCEL.\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=chat_template,\n",
    "    output_parser=parser\n",
    ")\n",
    "\n",
    "# Define the input variables for the human message and format instructions\n",
    "input_data = {\n",
    "    \"mood\": \"adventurous\",\n",
    "    \"genre\": \"action\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "}\n",
    "# Invoke the chain\n",
    "# The `.invoke()` method of LLMChain returns a dictionary, so we access the output via the \"text\" key.\n",
    "# The output is a Pydantic object, thanks to the parser.\n",
    "result = chain.invoke(input_data)\n",
    "\n",
    "# The output from the chain is automatically parsed into our Pydantic model.\n",
    "movie_suggestion = result['text']\n",
    "\n",
    "# Print the result\n",
    "print(\"Chain output type:\", type(movie_suggestion))\n",
    "print(movie_suggestion)\n",
    "print(f\"\\nTitle: {movie_suggestion.title}\")\n",
    "print(f\"Genre: {movie_suggestion.genre}\")\n",
    "print(f\"Reason: {movie_suggestion.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e8317-b44b-4d6d-9d0f-8a3ab7db1193",
   "metadata": {},
   "source": [
    "## Simple Sequential Chain and Sequential Chain  (depricated)\n",
    "A sequential chain is more advanced and links several steps together, where the output of one step becomes the input for the next\n",
    "\n",
    "#### Example: Blog content generation workflow\n",
    "* **Chain 1**: Takes a topic and generates a bullet-point outline.\n",
    "* **Chain 2**: Takes the outline and writes a full blog post.\n",
    "* **Chain 3**: Takes the blog post and creates a\n",
    "summary\n",
    "\n",
    "**Note**: SequentialChain and SimpleSequentialChain is deprecated in favor of the LangChain Expression Language (LCEL).\n",
    "\n",
    "<table>\n",
    "      <thead>\n",
    "        <tr>\n",
    "          <th>Feature</th>\n",
    "          <th>SimpleSequentialChain</th>\n",
    "          <th>SequentialChain</th>\n",
    "        </tr>\n",
    "      </thead>\n",
    "      <tbody>\n",
    "        <tr>\n",
    "          <td>Input/Output</td>\n",
    "          <td>Each step has a single input and a single output. <br>The output of one step is implicitly passed as the input to the next.</td>\n",
    "          <td>Allows multiple named inputs and outputs at each step, <br>giving you more explicit control over data flow.</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "          <td>Workflow</td>\n",
    "          <td>Ideal for simple, linear workflows where each<br> step is directly dependent on the output of the previous one. <br>This is sufficient for many basic use cases.</td>\n",
    "          <td> step is directly dependent on the output<br> of the previous one. This is sufficient for many basic use cases.<br>\tBuilt for complex workflows where you might need to use<br> the output from a previous step in multiple subsequent steps,<br> or combine multiple inputs before a step.</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "          <td>Variable Passing</td>\n",
    "          <td>Automatically handles the variable <br>passing between steps, making it simple<br> to set up. It is less flexible but requires less configuration.</td>\n",
    "          <td>Requires you to explicitly define and <br>map the input and output variables for each step.<br> This offers greater flexibility and control over how data moves through the chain.</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "          <td>Use case</td>\n",
    "          <td>A simple step process</td>\n",
    "          <td>A multi-step process with branching logic</td>\n",
    "        </tr>\n",
    "      </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07569aa6-245d-4c19-9934-ee0d150ed646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292ff1c-5064-4150-a005-3ad6dc3b1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the first chain: Create an outline\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_outline = PromptTemplate.from_template(\n",
    "    \"Create a bullet-point outline for a blog post about {topic}.\"\n",
    ")\n",
    "chain_outline = LLMChain(llm=llm, prompt=prompt_outline)\n",
    "\n",
    "\n",
    "# 3. Define the second chain: Write the blog post\n",
    "prompt_post = PromptTemplate.from_template(\n",
    "    \"Write a detailed blog post based on this outline: {outline}\"\n",
    ")\n",
    "chain_post = LLMChain(llm=llm, prompt=prompt_post)\n",
    "\n",
    "\n",
    "# 4. Define the third chain: Summarize the post\n",
    "prompt_summary = PromptTemplate.from_template(\n",
    "    \"Summarize this blog post: {post}\"\n",
    ")\n",
    "chain_summary = LLMChain(llm=llm, prompt=prompt_summary)\n",
    "\n",
    "\n",
    "# 5. Combine the chains into a SimpleSequentialChain\n",
    "final_chain = SimpleSequentialChain(\n",
    "    chains=[chain_outline, chain_post, chain_summary], verbose=True\n",
    ")\n",
    "\n",
    "# 6. Invoke the final chain\n",
    "final_result = final_chain.invoke(\"using AI in art\")\n",
    "print(\"\\n--- Final Summary ---\")\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18e203-ba92-4097-9190-2362a591aa1b",
   "metadata": {},
   "source": [
    "## Langchain Expression Language\n",
    "**The LangChain Expression Language (LCEL)** is a declarative way to compose components. It was developed to address limitations of the old Chain class system. LCEL provides several advantages:\n",
    "* **Streaming Support**: LCEL makes it easier to stream outputs from each step of your sequence, even when you are chaining calls. The older chains required using callbacks for streaming.\n",
    "* **Parallelism**: You can easily run multiple parts of a chain in parallel with LCEL, which can improve the performance of your application.\n",
    "* **Debugging and Visibility**: LCEL provides better  visibility into the structure of your application, making it easier to debug and inspect the intermediate steps of a chain.\n",
    "* **Standard Interface**: LCEL introduced the Runnable protocol, a standard interface that all components adhere to. SimpleSequentialChain was an early implementation of this concep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73479d-ab90-45f4-89fe-22a7f62fee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableSequence\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334b01d-9beb-42ba-91bb-af73d19c5e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the LLM and output parser\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 2. Define the prompt templates\n",
    "prompt_outline = ChatPromptTemplate.from_template(\n",
    "    \"Create a bullet-point outline for a blog post about {topic}.\"\n",
    ")\n",
    "\n",
    "prompt_post = ChatPromptTemplate.from_template(\n",
    "    \"Write a detailed blog post based on this outline: {outline}\"\n",
    ")\n",
    "\n",
    "prompt_summary = ChatPromptTemplate.from_template(\n",
    "    \"Summarize this blog post: {post}\"\n",
    ")\n",
    "# 3. Define the individual chains using LCEL\n",
    "chain_outline = prompt_outline | llm | parser\n",
    "chain_post = prompt_post | llm | parser\n",
    "chain_summary = prompt_summary | llm | parser\n",
    "\n",
    "# 4. Construct the overall chain\n",
    "overall_chain = (\n",
    "    # Step 1: Generate the outline\n",
    "    # Pass through the original input `topic` and add the `outline` key\n",
    "    RunnablePassthrough.assign(outline=chain_outline)\n",
    "    # Step 2: Generate the post\n",
    "    # Pass through the new input (with 'topic' and 'outline') and add the 'post' key\n",
    "    | RunnablePassthrough.assign(post=chain_post)\n",
    "    # Step 3: Generate the summary\n",
    "    # Pass through the input (with 'topic', 'outline', and 'post') and add the 'summary' key\n",
    "    | RunnablePassthrough.assign(summary=chain_summary)\n",
    ")\n",
    "# 5. Invoke the chain\n",
    "result = overall_chain.invoke({\"topic\": \"LangChain Expression Language (LCEL)\"})\n",
    "\n",
    "# 6. Access the results\n",
    "print(\"--- Outline ---\")\n",
    "print(result[\"outline\"])\n",
    "print(\"\\n--- Blog Post ---\")\n",
    "print(result[\"post\"])\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(result[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b4c0a7-9bf7-4713-8174-71c9ec5dd40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parser\n",
    "parser = PydanticOutputParser(pydantic_object=MovieSuggestion)\n",
    "\n",
    "# Define the chat prompt template with multiple message roles\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # The system message sets the overall behavior and instructions\n",
    "        (\"system\", \"You are a helpful AI assistant that recommends a movie based on the user's mood. Always provide a single recommendation. {format_instructions}\"),\n",
    "\n",
    "        # This is a placeholder for previous conversation turns (optional)\n",
    "        # It's not strictly a part of the *current* prompt but shows how history is inserted\n",
    "        # from langchain.prompts import MessagesPlaceholder\n",
    "        # MessagesPlaceholder(variable_name=\"chat_history\")\n",
    "\n",
    "        # Example of a previous conversation turn (simulating what might have happened)\n",
    "        # This is a static example, but in a real app, you'd insert actual history here\n",
    "        (\"ai\", \"I'm good, thanks for asking! What kind of mood are you in?\"),\n",
    "\n",
    "        # Template for the current user's input\n",
    "        (\"human\", \"I'm in the mood for something {mood}. I want my recommendation to be a {genre}.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the chat model (e.g., GPT-4)\n",
    "# You need to have your OpenAI API key set as an environment variable\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0, api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create the chain using LCEL syntax\n",
    "# This is the modern, more robust alternative to LLMChain\n",
    "chain = chat_template | llm | parser\n",
    "\n",
    "# Define the input variables\n",
    "input_data = {\"mood\": \"adventurous\", \"genre\": \"action\", \"format_instructions\": parser.get_format_instructions()}\n",
    "\n",
    "# Invoke the chain to get the structured output\n",
    "result = chain.invoke(input_data)\n",
    "\n",
    "# Print the result\n",
    "print(result)\n",
    "print(f\"\\nTitle: {result.title}\")\n",
    "print(f\"Genre: {result.genre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb479f28-612d-4ff3-a61d-72b930a5a46c",
   "metadata": {},
   "source": [
    "## Route Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eba5a9-c298-4f2d-906b-ffecfe8dde40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Dict, Callable\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch, RunnableMap\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00423a-6ac7-4be5-8615-18cd8052c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_simple_chain(system_msg: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([(\"system\", system_msg), (\"user\", \"{input}\")])\n",
    "    return prompt | llm | StrOutputParser()\n",
    "\n",
    "destinations: Dict[str, Callable] = {\n",
    "    \"math\":    make_simple_chain(\"You are a careful math solver.\"),\n",
    "    \"code\":    make_simple_chain(\"You are a helpful coding assistant. Provide runnable code.\"),\n",
    "    \"general\": make_simple_chain(\"You are a concise general assistant.\")\n",
    "}\n",
    "\n",
    "# ----- reusable router (no format_instructions needed) -----\n",
    "class Route(BaseModel):\n",
    "    destination: Literal[tuple(destinations.keys())] = Field(\n",
    "        description=\"Best handler for the user's query.\"\n",
    "    )\n",
    "    reason: str\n",
    "\n",
    "router_system = (\n",
    "    \"You are a router. Pick exactly one destination for the user's query \"\n",
    "    f\"from: {', '.join(destinations.keys())}.\"\n",
    ")\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", router_system), (\"user\", \"{input}\")]\n",
    ")\n",
    "\n",
    "# 'with_structured_output' makes the LLM return a Route object via native tool/JSON\n",
    "router_chain = router_prompt | llm.with_structured_output(Route)\n",
    "\n",
    "# ----- branch wiring (tiny and reusable) -----\n",
    "def make_routed_chain():\n",
    "    router_and_input = RunnableMap(route=router_chain, input=lambda x: x[\"input\"])\n",
    "    return router_and_input | RunnableBranch(\n",
    "        *((lambda x, k=k: x[\"route\"].destination == k, v) for k, v in destinations.items()),\n",
    "        # default fallback:\n",
    "        destinations[\"general\"]\n",
    "    )\n",
    "\n",
    "routed_chain = make_routed_chain()\n",
    "\n",
    "# use:\n",
    "routed_chain.invoke({\"input\": \"Derivative of x^3 + 2x?\"})\n",
    "routed_chain.invoke({\"input\": \"Write a Python function to merge two sorted lists.\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
