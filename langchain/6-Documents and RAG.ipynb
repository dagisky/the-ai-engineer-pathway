{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e2221eb-a39c-4de3-9bc5-7519d694c28a",
   "metadata": {},
   "source": [
    "## Documents in LangChain\n",
    "\n",
    "In LangChain, the Document is the fundamental building block for working with external knowledge. Whenever you ingest data from a file, webpage, or database, LangChain represents that piece of content as a Document object.\n",
    "\n",
    "A Document has two main components:\n",
    "\n",
    "* **page_content (str)**\n",
    "* **metadata (dict)**\n",
    "\n",
    "The raw text you want the LLM to use.\n",
    "Metadata is essential for traceability (e.g., citing the page number in an answer, knowing which file the text came from).\n",
    "\n",
    "## Why Documents Are Important\n",
    "\n",
    "* **Uniformity** → Regardless of source (PDF, web, SQL database), everything becomes a standardized Document.\n",
    "\n",
    "* **Traceability** → You always know where the LLM’s answer is coming from.\n",
    "\n",
    "* **Flexibility** → Metadata can store rich details (e.g., author, timestamp, topic) that help filtering and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1427b00a-2adf-4845-a4c8-04d2c59ed6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader, TextLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f6feff-6f22-4833-8e72-9440fa2c65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Single PDF\n",
    "# pdf_docs = PyPDFLoader(\"sample.pdf\").load()\n",
    "\n",
    "# 2. Web page\n",
    "web_docs = WebBaseLoader(\"https://python.langchain.com\").load()\n",
    "\n",
    "# 3. Single text file\n",
    "# txt_docs = TextLoader(\"notes.txt\", encoding=\"utf-8\").load()\n",
    "\n",
    "# 4. Whole folder (all Markdown files)\n",
    "# dir_docs = DirectoryLoader(\"./docs\", glob=\"**/*.md\").load()\n",
    "\n",
    "# 5. Minimal in-memory doc (for demo)\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"LangChain streamlines LLM apps. It has loaders, splitters, embeddings, vectorstores, and retrievers.\",\n",
    "        metadata={\"source\": \"in_memory\"}\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f83479-d0b5-4128-89bc-40d81b73653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: LangChain makes it easy to work with documents by\n",
      "Chunk 2: loading, splitting, embedding, and retrieving\n",
      "Chunk 3: them for LLM-powered applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\"LangChain makes it easy to work with documents by\n",
    "loading, splitting, embedding, and retrieving them for LLM-powered applications.\"\"\"\n",
    "\n",
    "# Define splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "# Split into smaller docs\n",
    "docs = splitter.create_documents([text])\n",
    "\n",
    "for i, d in enumerate(docs, 1):\n",
    "    print(f\"Chunk {i}: {d.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8368dc6-d1e4-4f07-8953-ea5fab8cbe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: LangChain makes it easy to work with documents by\n",
      "Metadata: {'source': 'intro.txt', 'author': 'LangChain Team', 'chunk_id': 1}\n",
      "------------------------------------------------------------\n",
      "Chunk 2: by loading, splitting, embedding, and retrieving\n",
      "Metadata: {'source': 'intro.txt', 'author': 'LangChain Team', 'chunk_id': 2}\n",
      "------------------------------------------------------------\n",
      "Chunk 3: them for LLM-powered applications.\n",
      "Metadata: {'source': 'intro.txt', 'author': 'LangChain Team', 'chunk_id': 3}\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Original Document\n",
    "doc = Document(\n",
    "    page_content=(\n",
    "        \"LangChain makes it easy to work with documents by loading, \"\n",
    "        \"splitting, embedding, and retrieving them for LLM-powered applications.\"\n",
    "    ),\n",
    "    metadata={\"source\": \"intro.txt\", \"author\": \"LangChain Team\"}\n",
    ")\n",
    "\n",
    "# Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "# Split into smaller Document objects\n",
    "chunks = splitter.split_documents([doc])\n",
    "\n",
    "# Add chunk info to metadata\n",
    "for idx, chunk in enumerate(chunks, start=1):\n",
    "    chunk.metadata[\"chunk_id\"] = idx\n",
    "\n",
    "# Print results\n",
    "for c in chunks:\n",
    "    print(f\"Chunk {c.metadata['chunk_id']}: {c.page_content}\")\n",
    "    print(\"Metadata:\", c.metadata)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3cb458-c3a9-421a-9115-39eb895a921a",
   "metadata": {},
   "source": [
    "# LangChain Text Embeddings\n",
    "An embedding is a vector representation of text. It captures the semantic meaning of words, sentences, or documents, so similar texts are close to each other in vector space.\n",
    "\n",
    "In LangChain, embeddings are used to:\n",
    "\n",
    "* **Enable semantic search** (find conceptually similar text).\n",
    "* **Power retrieval-augmented** generation (RAG) workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5775b05d-8ab1-4065-a8d6-87e8df8c9767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536 dimensions\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = OpenAIEmbeddings(api_key=os.environ[\"OPEN_API_KEY\"])\n",
    "\n",
    "# Convert text to vector\n",
    "vector = embedding_model.embed_query(\"LangChain makes working with LLMs easier\")\n",
    "print(len(vector), \"dimensions\") # -> e.g., 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acca2e1-e574-4b7e-820d-43f33864f334",
   "metadata": {},
   "source": [
    "# 🗄️ Vector Stores in LangChain\n",
    "\n",
    "A Vector Store is a special type of database designed to store and query vector embeddings (numerical representations of text). Instead of exact keyword matches, vector stores allow semantic similarity search, meaning you can retrieve chunks of text that are conceptually related to a query.\n",
    "\n",
    "In LangChain, vector stores are the backbone of retrieval-augmented generation (RAG) workflows.\n",
    "\n",
    "###  🔑 Why Vector Stores Matter\n",
    "\n",
    "* **Efficient search** → optimized for high-dimensional vector lookups.\n",
    "* **Semantic retrieval** → retrieves relevant documents by meaning, not keywords.\n",
    "* **Metadata storage** → keeps track of the original source, author, page, etc.\n",
    "* **Scalability** → can handle millions of embeddings with low latency.\n",
    "\n",
    "### 📚 Popular Vector Stores in LangChain\n",
    "\n",
    "* **Chroma** → default, simple, and local.\n",
    "* **FAISS** → Facebook AI Similarity Search (fast, offline).\n",
    "* **Pinecone** → managed cloud vector DB.\n",
    "* **Weaviate, Qdrant, Milvus** → other scalable open-source options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1660556a-e0d9-4700-b4a3-a18993eaf697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "401f3dc9-b8f8-45cd-b19b-f16623dff7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector stores enable semantic search. -> {'id': 2}\n",
      "LangChain integrates with OpenAI. -> {'id': 1}\n"
     ]
    }
   ],
   "source": [
    "# 1. Embedding model\n",
    "embeddings = OpenAIEmbeddings(api_key=os.environ[\"OPEN_API_KEY\"])\n",
    "\n",
    "# 2. Example documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain integrates with OpenAI.\", metadata={\"id\": 1}),\n",
    "    Document(page_content=\"Vector stores enable semantic search.\", metadata={\"id\": 2}),\n",
    "]\n",
    "\n",
    "# 3. Create vector store\n",
    "vectorstore = Chroma.from_documents(docs, embedding=embeddings)\n",
    "\n",
    "# 4. Query the store\n",
    "results = vectorstore.similarity_search(\"How to do semantic search?\", k=2)\n",
    "\n",
    "for r in results:\n",
    "    print(r.page_content, \"->\", r.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc838782-3b3e-45ef-bf11-1ae2cd18b7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
